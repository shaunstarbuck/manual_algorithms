{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import cholesky, det, lstsq\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, noise=0.2):\n",
    "    return -np.sin(3*X) - X**2 + 0.7*X + noise * np.random.randn(*X.shape)\n",
    "\n",
    "X_train = np.array([[-0.9], [0], [0.5], [0.9], [1.1]])\n",
    "Y_train = f(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From gausian process\n",
    "noise = 0.4\n",
    "# Noisy training data\n",
    "X_train = np.arange(-3, 4, 1).reshape(-1, 1)\n",
    "Y_train = np.sin(X_train) + noise * np.random.randn(*X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 4, 1, 0, 1, 4, 9], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_train**2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF_noise_kernel(X1, X2, l = 1.0, sigma_f = 1.0, sigma_y = 1.0):\n",
    "    '''\n",
    "    Isotropic squared exponential kernal (radial basis function kernel). Computes\n",
    "    a covariance matrix from points in X1 and X2.\n",
    "    Args:\n",
    "        X1: Array of m points \n",
    "        X2: array of n points\n",
    "        sigma_f: variance of the Gaussian Process\n",
    "        sigma_y: noise variance\n",
    "    Returns:\n",
    "        Covariance matrix(m X n).\n",
    "    '''\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1,1) + np.sum(X2**2, 1) -  2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist) + sigma_y**2 * np.eye(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive(X_s, X_train, Y_train, l = 1.0, sigma_f = 1.0, sigma_y = 1e-8, kernel = RBF_noise_kernel):\n",
    "    '''\n",
    "    Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "    from m training data X_train and Y_train and n new inputs X_s.\n",
    "    \n",
    "    Args:\n",
    "        X_s: New input locations (n x d).\n",
    "        X_train: Training locations (m x d).\n",
    "        Y_train: Training targets (m x 1).\n",
    "        l: Kernel length parameter.\n",
    "        sigma_f: Kernel vertical variation parameter.\n",
    "        sigma_y: Noise parameter.\n",
    "        kernel: kernel function for the GP\n",
    "    Returns:\n",
    "        Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "    '''\n",
    "    #Create kernel with noise added along the matrix diagonal\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "    #Get covariance of new inputs from X_train\n",
    "    K_s = kernel(X_train, X_s, l , sigma_f)\n",
    "    #Get Covariance of new inputs, with noise\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\n",
    "    #get inverse of X_train covariance matrix\n",
    "    K_inv = inv(K)\n",
    "    \n",
    "    #Calculate mean of the posterior predictive distribution \n",
    "    mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "    \n",
    "    #Calculate covariance matrix of the posterior predictive distribution \n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "    \n",
    "    return mu_s, cov_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_full_fn(X_train, Y_train, naive=True):\n",
    "    '''\n",
    "    Returns a function that computes the negative log marginal\n",
    "    likelihood for training data X_train and Y_train and an unknown \n",
    "    noise level.\n",
    "    \n",
    "    Args:\n",
    "        X_train: training locations (m x d).\n",
    "        Y_train: training targets (m x 1).\n",
    "        naive: if True use a naive implementation of Eq. (7), if \n",
    "               False use a numerically more stable implementation. \n",
    "        \n",
    "    Returns:\n",
    "        Minimization objective.\n",
    "    '''\n",
    "    def nll_naive(theta):\n",
    "        # Naive implementation of the equation for log marginal likeliehood. Works well for simple examples \n",
    "        # but is numerically less stable compared to \n",
    "        # the implementation in nll_stable below.\n",
    "        K = RBF_noise_kernel(X_train, X_train, l=theta[0], sigma_f=theta[1], sigma_y=theta[2])\n",
    "        return 0.5 * np.log(det(K)) + \\\n",
    "               0.5 * Y_train.T.dot(inv(K).dot(Y_train)) + \\\n",
    "               0.5 * len(X_train) * np.log(2*np.pi)\n",
    "\n",
    "    def nll_stable(theta):\n",
    "        # Numerically more stable implementation of  equation for log marginal likeliehood as described\n",
    "        # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "        # 2.2, Algorithm 2.1.\n",
    "        K = RBF_noise_kernel(X_train, X_train, l=theta[0], sigma_f=theta[1], sigma_y=theta[2])\n",
    "        L = cholesky(K)\n",
    "        return np.sum(np.log(np.diagonal(L))) + \\\n",
    "               0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "               0.5 * len(X_train) * np.log(2*np.pi)\n",
    "    \n",
    "    if naive:\n",
    "        return nll_naive\n",
    "    else:\n",
    "        return nll_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3269925 , 2.90431074, 1.87032437],\n",
       "       [0.21828521, 2.69015686, 0.704197  ],\n",
       "       [0.97760723, 0.44340326, 2.54062448],\n",
       "       [2.23805631, 1.96205646, 1.5922256 ],\n",
       "       [2.97053554, 2.4440606 , 1.30513295],\n",
       "       [0.02356643, 1.05392543, 0.21336335],\n",
       "       [0.18284152, 2.23634398, 0.76604553],\n",
       "       [2.64453164, 2.99947593, 0.8744783 ],\n",
       "       [1.92692148, 1.46257513, 1.30458259],\n",
       "       [1.08338357, 2.28104967, 1.55716542]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 3, size=(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(nll_full_fn(X_train, Y_train), [1,1,1], \n",
    "              bounds = ((1e-5, None), (1e-5, None),(1e-5, None)),\n",
    "              method = 'L-BFGS-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44775204e+00, 1.33911646e+00, 1.00000000e-05])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\higgleop\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "# Minimize the negative log-likelihood w.r.t. parameters l, sigma_f, and sigma_y.\n",
    "# Can run the minimization several times with different\n",
    "# initializations to avoid local minima.\n",
    "dim = 3\n",
    "min_val = 10\n",
    "min_x = None\n",
    "n_restarts = 20\n",
    "\n",
    "# Find the best optimum by starting from n_restart different random points.\n",
    "for x0 in np.random.uniform(0, 2, size=(n_restarts, dim)):      \n",
    "    res = minimize(nll_full_fn(X_train, Y_train, naive= False), x0 = x0, \n",
    "        bounds = ((1e-5, None), (1e-5, None),(1e-5, None)),\n",
    "        method = 'L-BFGS-B')\n",
    "    #print(res.fun)\n",
    "    if res.fun < min_val:\n",
    "        min_val = res.fun[0]\n",
    "        min_x = res.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44775361e+00, 1.33912055e+00, 1.00000000e-05])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.35523829]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 0], [1,1]]\n",
    "b = [[2, 3], [1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [3, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
